{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tilakraj-B/cardiac-disease/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3BIKvZCbVjIg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRsbK4BQZlkp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vaS5QdR-hKTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9243254-b937-4b22-daf9-8092c44d1a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0NLwWNEthU-u"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/acdc_dataset.zip\"  # Update this path\n",
        "extract_folder = \"/content/acdc_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH1qeuBnhyDM",
        "outputId": "714cc08d-4b00-4a02-8d54-8334de1ee40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already extracted.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Extract the dataset if not already extracted\n",
        "if not os.path.exists(extract_folder):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(dataset_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0EAoR5_piNUe"
      },
      "outputs": [],
      "source": [
        "def load_nii_file(filepath):\n",
        "    img = nib.load(filepath)\n",
        "    data = img.get_fdata()\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ae5MWnmJiQlb"
      },
      "outputs": [],
      "source": [
        "# Step 5: Load and visualize a sample image (Modify the path based on extracted files)\n",
        "base_dir = os.path.join(extract_folder,\"database\")\n",
        "training_folder = os.path.join(extract_folder,\"database\",\"training\")\n",
        "testing_folder = os.path.join(extract_folder,\"database\",\"testing\")\n",
        "train_patients = [f'patient{i:03d}' for i in range(1, 50)]\n",
        "test_patients = [f'patient{i:03d}' for i in range(1, 50)]\n",
        "\n",
        "# info_file_path = os.path.join(patient_folder, \"Info.cfg\")\n",
        "\n",
        "# if os.path.exists(info_file_path):\n",
        "#     with open(info_file_path, \"r\") as file:\n",
        "#         info_content = file.read()\n",
        "#     print(\"Contents of Info.cfg:\\n\")\n",
        "#     print(info_content)\n",
        "# else:\n",
        "#     print(\"Info.cfg not found. Check the path!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LGIFbM82rcWG"
      },
      "outputs": [],
      "source": [
        "# Display all the images of a patient\n",
        "\n",
        "def display_nifti_slices(nifti_path):\n",
        "    nifti_img = nib.load(nifti_path)  # Load NIfTI file\n",
        "    image_data = nifti_img.get_fdata()  # Convert to NumPy array\n",
        "\n",
        "    print(f\"\\nDisplaying slices for {os.path.basename(nifti_path)}\")\n",
        "    print(f\"Image shape: {image_data.shape}\")  # Print shape\n",
        "\n",
        "    dim = image_data.ndim  # Get number of dimensions\n",
        "    if dim == 2:  # Case: 2D Image\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(image_data, cmap=\"gray\")\n",
        "        plt.title(f\"{os.path.basename(nifti_path)} - 2D Image\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    elif dim == 3:  # Case: 3D Image (H, W, D)\n",
        "        num_slices = image_data.shape[2]\n",
        "        for i in range(num_slices):\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(image_data[:, :, i], cmap=\"gray\")\n",
        "            plt.title(f\"{os.path.basename(nifti_path)} - Slice {i+1}/{num_slices}\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "    elif dim == 4:  # Case: 4D Image (H, W, D, T)\n",
        "        num_slices = image_data.shape[2]\n",
        "        num_timeframes = image_data.shape[3]\n",
        "        for t in range(num_timeframes):  # Loop over time\n",
        "            for i in range(num_slices):  # Loop over depth\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                plt.imshow(image_data[:, :, i, t], cmap=\"gray\")\n",
        "                plt.title(f\"{os.path.basename(nifti_path)} - Time {t+1}/{num_timeframes} - Slice {i+1}/{num_slices}\")\n",
        "                plt.axis(\"off\")\n",
        "                plt.show()\n",
        "\n",
        "    else:\n",
        "        print(f\"Unsupported image dimension: {dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": true,
        "id": "0JnBpp3DBzVn"
      },
      "outputs": [],
      "source": [
        "# # displaying dimension and shape of the imagedatabase\n",
        "# for i in range(1,100):\n",
        "#   patient_folder = os.path.join(training_folder, f\"patient{i:03d}\")\n",
        "#   for filename in sorted(os.listdir(patient_folder)):\n",
        "#     if filename.endswith(\".nii\"):\n",
        "#       nifti_path = os.path.join(patient_folder, filename)\n",
        "#       # display_nifti_slices(nifti_path)\n",
        "#       nifti_img = nib.load(nifti_path)  # Load NIfTI file\n",
        "#       image_data = nifti_img.get_fdata()  # Convert to NumPy array\n",
        "#       dim = image_data.ndim\n",
        "#       # print(filename + \" : \" + str(dim) + \" dimensions\" + str(image_data.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZgCIXAAB3EM4"
      },
      "outputs": [],
      "source": [
        "def load_patient_data(patient_folder):\n",
        "    # Get all .nii files in the folder\n",
        "    files = [f for f in os.listdir(patient_folder) if f.endswith('.nii') and not f.endswith('4d.nii')]\n",
        "\n",
        "    # Separate image and ground truth files\n",
        "    image_files = [f for f in files if not f.endswith('_gt.nii')]\n",
        "    gt_files = [f for f in files if f.endswith('_gt.nii')]\n",
        "\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for img_file, gt_file in zip(sorted(image_files), sorted(gt_files)):\n",
        "        img_data = load_nii_file(os.path.join(patient_folder, img_file))\n",
        "        gt_data = load_nii_file(os.path.join(patient_folder, gt_file))\n",
        "\n",
        "        #iterating through each slice in the model\n",
        "        for slice_idx in range(img_data.shape[2]):\n",
        "            img_slice = img_data[..., slice_idx]\n",
        "            gt_slice = gt_data[..., slice_idx]\n",
        "\n",
        "            # Normalize and resize if needed\n",
        "            img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())\n",
        "            gt_slice = (gt_slice > 0).astype(np.float32)  # Binarize if needed\n",
        "\n",
        "            # Resize to make dimensions divisible by 16 for U-Net (optional)\n",
        "            img_slice = tf.image.resize(img_slice[..., np.newaxis], [224, 256])\n",
        "            gt_slice = tf.image.resize(gt_slice[..., np.newaxis], [224, 256])\n",
        "\n",
        "            images.append(img_slice.numpy())\n",
        "            masks.append(gt_slice.numpy())\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XA9Sdc_13eAt"
      },
      "outputs": [],
      "source": [
        "def load_dataset(root_folder, patient_range):\n",
        "    all_images = []\n",
        "    all_masks = []\n",
        "\n",
        "    for patient_id in patient_range:\n",
        "        patient_folder = os.path.join(root_folder, f\"patient{patient_id:03d}\")\n",
        "        if os.path.exists(patient_folder):\n",
        "            images, masks = load_patient_data(patient_folder)\n",
        "            all_images.extend(images)\n",
        "            all_masks.extend(masks)\n",
        "\n",
        "    return np.array(all_images), np.array(all_masks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "F9vi-tQ_rWjS"
      },
      "outputs": [],
      "source": [
        "def unet_model(input_size=(224, 256, 1)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsample path\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    # Upsample path\n",
        "    up6 = Conv2D(512, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop5))\n",
        "    merge6 = concatenate([drop4, up6], axis=3)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6))\n",
        "    merge7 = concatenate([conv3, up7], axis=3)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv7))\n",
        "    merge8 = concatenate([conv2, up8], axis=3)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv8))\n",
        "    merge9 = concatenate([conv1, up9], axis=3)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6cqUVbyhcs4S"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    # Load training data\n",
        "    train_images, train_masks = load_dataset(training_folder, range(1, 101))\n",
        "\n",
        "    # Split into training and validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        train_images, train_masks, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = unet_model()\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.IoU(num_classes=2, target_class_ids=[1])])\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
        "        tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
        "    ]\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=16,\n",
        "        epochs=100,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fqySk5hFBLKP"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model):\n",
        "    # Load test data\n",
        "    test_images, test_masks = load_dataset(testing_folder, range(101, 151))\n",
        "\n",
        "    # Evaluate\n",
        "    results = model.evaluate(test_images, test_masks)\n",
        "    print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}, Test IoU: {results[2]}\")\n",
        "\n",
        "    # Optionally visualize some predictions\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    preds = model.predict(test_images[:5])\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(10):\n",
        "        plt.subplot(3, 5, i+1)\n",
        "        plt.imshow(test_images[i].squeeze(), cmap='gray')\n",
        "        plt.title('Input')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(3, 5, i+6)\n",
        "        plt.imshow(test_masks[i].squeeze(), cmap='gray')\n",
        "        plt.title('Ground Truth')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(3, 5, i+11)\n",
        "        plt.imshow(preds[i].squeeze() > 0.5, cmap='gray')\n",
        "        plt.title('Prediction')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQHr49Db6il",
        "outputId": "f5f28908-70f1-4de1-c970-4089edbab579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113s/step - accuracy: 0.7841 - io_u_3: 0.0000e+00 - loss: 11.9001  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2372s\u001b[0m 125s/step - accuracy: 0.7899 - io_u_3: 0.0000e+00 - loss: 11.6650 - val_accuracy: 0.9523 - val_io_u_3: 0.0000e+00 - val_loss: 0.2341\n",
            "Epoch 2/10\n",
            "\u001b[1m 3/19\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32:07\u001b[0m 120s/step - accuracy: 0.9547 - io_u_3: 0.0000e+00 - loss: 0.2239"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    model, history = train_model()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    evaluate_model(model)\n",
        "\n",
        "    # Optionally save the model\n",
        "    model.save('unet_mri_segmentation.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNfRtDl9iWCKIQZQo2Qujh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}